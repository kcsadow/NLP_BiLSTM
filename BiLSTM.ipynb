{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKuTb2_4hHX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5273e8b-0401-4df3-d499-fa15f4eab10d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#drive_folder = \"gdrive/My Drive/CS6741 Replication Project/\" \n",
        "drive_folder = \"gdrive/My Drive/CS6741 - Topics in Natural Language Processing and Machine Learning/CS6741 Replication Project/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwjf3IQkhZnQ"
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torchtext import datasets\n",
        "from torchtext.legacy import data\n",
        "import re\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import spacy \n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wth4dccFhb0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a96a4c-6cfc-4a1c-9979-1992ee2145f9"
      },
      "source": [
        "#@Glove files\n",
        "glove = torchtext.vocab.GloVe(name='6B',dim=300)\n",
        "\n",
        "print(glove.vectors.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 399200/400000 [00:37<00:00, 10665.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([400000, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4M5mXMNht3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "06125a22-6003-4e9c-c938-0c45af1c19d0"
      },
      "source": [
        "#@Sentiment files (2 Class from https://github.com/clairett/pytorch-sentiment-classification)\n",
        "colnames=['review', 'sentiment'] \n",
        "train=pd.read_csv(drive_folder+\"data/SST2/train.tsv\", sep = '\\t', names=colnames, header=None)\n",
        "dev=pd.read_csv(drive_folder+\"data/SST2/dev.tsv\", sep = '\\t', names=colnames, header=None)\n",
        "test=pd.read_csv(drive_folder+\"data/SST2/test.tsv\", sep = '\\t', names=colnames, header=None)\n",
        "\n",
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one long string of cliches</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>if you 've ever entertained the notion of doin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>k 19 exploits our substantial collective fear ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it 's played in the most straight faced fashio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>there is a fabric of complex ideas here , and ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0                         one long string of cliches          0\n",
              "1  if you 've ever entertained the notion of doin...          0\n",
              "2  k 19 exploits our substantial collective fear ...          0\n",
              "3  it 's played in the most straight faced fashio...          0\n",
              "4  there is a fabric of complex ideas here , and ...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZVPocnohyC4"
      },
      "source": [
        "#@Update preprocessing with our thoughts\n",
        "contraction_dict = {\n",
        "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
        "    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "    \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
        "    \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i will\",  \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "    \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
        "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "def clean_contractions(text, contraction_dict):\n",
        "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "    for s in specials:\n",
        "        text = text.replace(s, \"'\")\n",
        "    text = ' '.join([contraction_dict[t] if t in contraction_dict else t for t in text.split(\" \")])\n",
        "    return text\n",
        "\n",
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "punct_dict = {\n",
        "    \"‘\": \"'\",    \"₹\": \"e\",      \"´\": \"'\", \"°\": \"\",         \"€\": \"e\",\n",
        "    \"™\": \"tm\",   \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\",        \"—\": \"-\",\n",
        "    \"–\": \"-\",    \"’\": \"'\",      \"_\": \"-\", \"`\": \"'\",        '“': '\"',\n",
        "    '”': '\"',    '“': '\"',      \"£\": \"e\", '∞': 'infinity', 'θ': 'theta',\n",
        "    '÷': '/',    'α': 'alpha',  '•': '.', 'à': 'a',        '−': '-',\n",
        "    'β': 'beta', '∅': '',       '³': '3', 'π': 'pi'\n",
        "}\n",
        "def clean_special_chars(text, punct, punct_dict):\n",
        "    for p in punct_dict:\n",
        "        text = text.replace(p, punct_dict[p])\n",
        "    \n",
        "    for p in punct:\n",
        "        text = text.replace(p, f' {p} ')\n",
        "    \n",
        "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
        "    for s in specials:\n",
        "        text = text.replace(s, specials[s])\n",
        "    \n",
        "    return text\n",
        "\n",
        "#@Fix Stop words\n",
        "#import nltk\n",
        "#stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def preprocess_text(text, contraction_dict, punct, punct_dict):\n",
        "    clean_text=text.lower()\n",
        "    clean_text=clean_contractions(clean_text, contraction_dict)\n",
        "    clean_text=clean_special_chars(clean_text, punct, punct_dict)\n",
        "    clean_text=re.split('\\W+', clean_text)\n",
        "    #clean_text=[token for token in clean_text if token not in stopwords]  \n",
        "    return \" \".join(clean_text)\n",
        "\n",
        "preprocess_text(\"samhdbei. 2345324@@# !~~~ sdne @ dsecwAADEk. SDKM\",contraction_dict, punct, punct_dict)\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser','tagger','ner'])\n",
        "\n",
        "def tokenizer(s):\n",
        "    return [w.text.lower() for w in nlp(preprocess_text(s,contraction_dict, punct, punct_dict))]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19u_sU9a9BC0",
        "outputId": "f14219ae-22aa-4537-c186-50e402334642"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        " \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)   \n",
        "\n",
        "device=get_default_device()\n",
        "device"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io0RoRXc7hv4",
        "outputId": "99ef6a54-5bd4-4a8c-84e7-60d38f7e4f2e"
      },
      "source": [
        "# Ask Sasha about this section\n",
        "# - Glove + SST --> tensor batch size x review length \n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(tokenize = tokenizer)\n",
        "LABEL = torchtext.legacy.data.LabelField(dtype = torch.float)\n",
        "\n",
        "#reading again using tabular dataset\n",
        "datafields=[('review', TEXT),('sentiment', LABEL)]\n",
        "trn,val,tst=torchtext.legacy.data.TabularDataset.splits(path=drive_folder+\"data/SST2/\", train='train.tsv', validation='dev.tsv', test='test.tsv',format='tsv',skip_header=False, fields=datafields)\n",
        "\n",
        "#including ony top 30000 words from vocab, building vocab for train data \n",
        "TEXT.build_vocab(trn,max_size=30000,vectors='glove.6B.300d', unk_init=torch.Tensor.normal_)\n",
        "# extarcting these words from glove embeddings i.e. unique ids representing words should come from glove\n",
        "# for unkonwn or oov words, initialize them using normal distribution\n",
        "# change to higher dimensionaity vector later\n",
        "LABEL.build_vocab(trn)\n",
        "\n",
        "#@Fix Mappings (they are currently backwards, see next cell)\n",
        "LABEL.vocab.stoi\n",
        "\n",
        "#loop through trn and get a minibatch to work with - creates tensor X \n",
        "train_iterator,test_iterator, val_iterator=torchtext.legacy.data.BucketIterator.splits((trn,tst,val),batch_sizes=(10,10,10),sort_key =lambda x: len(x.review), sort_within_batch=False, device=device)\n",
        "print(len(train_iterator))# train batches\n",
        "print(len(val_iterator))# val batches"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "692\n",
            "88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tahHqo6STXwh",
        "outputId": "207edbbf-8066-42da-c74b-9634b0dfd85f"
      },
      "source": [
        "LABEL.vocab.stoi"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(None, {'0': 1, '1': 0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7a2jTcm9Ep4",
        "outputId": "bc6e4639-d954-4c68-e75d-1b046ace7923"
      },
      "source": [
        "def show_batch(dl):\n",
        "    for reviews, sentiments in dl:\n",
        "        print(reviews.T.shape)\n",
        "        print(sentiments.shape)\n",
        "        print(sentiments)\n",
        "        break\n",
        "        \n",
        "show_batch(train_iterator)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 37])\n",
            "torch.Size([10])\n",
            "tensor([1., 0., 1., 1., 0., 0., 0., 1., 0., 1.], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AhXBclth5pe"
      },
      "source": [
        "# Ask Sasha about this section\n",
        "# - Dimensions of hidden, final softmax (2 or 1)\n",
        "# - Role of cell?\n",
        "# - Why is hidden 3D?\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
        "                 output_dim, n_layers, bidirectional, batch_first, dropout):\n",
        "        \n",
        "        super(BiLSTM, self).__init__()\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # convert sparse 1 hot encoded vectors to embeddings (glove embedding will be used here)\n",
        "        \n",
        "        #hidden_dim -> takes in 300 embedding gives 20 features (if we set hyperparamter hidden_dim = 20)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim,\n",
        "                            num_layers=n_layers, batch_first=batch_first,\n",
        "                            dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim) # this is the W \n",
        "\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "      embedded = self.embedding(text)\n",
        "\n",
        "      embedded = self.dropout(embedded)\n",
        "\n",
        "      _, (hidden, _) = self.lstm(embedded)\n",
        "\n",
        "      #print('LSTM hidden shape', hidden.shape)\n",
        "      #now hidden is this x this\n",
        "      #maybe: 2xbatchxlength\n",
        "\n",
        "      #final hidden from left/right and concatenating\n",
        "      #print(hidden) \n",
        "      hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)  #hidden shape before concatenation: 4x10x100\n",
        "      #print(hidden)\n",
        "      #print('SHAPE',hidden.shape)\n",
        "\n",
        "      output = self.fc(hidden) \n",
        "      #print('Linear output shape', output.shape)\n",
        "\n",
        "      #return torch.log_softmax(output, dim=-1) #this is the final phi from class - HAS to be log_softmax\n",
        "      return output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi3zrOZlvbJq"
      },
      "source": [
        "#@Setting configurations and instantiating model\n",
        "\n",
        "# Questions:\n",
        "# - Why define embedding paramter outside class?\n",
        "# - Why can we use this section after instantiating model?\n",
        "# (i.e. why do this when we defined paramter in init?)\n",
        "\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 100 # Ask Sasha\n",
        "output_dim =  2\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5 #0 by default - won't accept this for some unknown reffason\n",
        "batch_first=True\n",
        "\n",
        "#model = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, batch_first) # dropout - set to 0 by default\n",
        "model = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, batch_first, dropout)\n",
        "model.cuda()\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0002) # Based on class code\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=10e-6)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, weight_decay=10e-6) # Based on Bastings et al. 2020\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.009, weight_decay=10e-6)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bvPYmAzCXBq",
        "outputId": "87c1a9cf-a3d4-418c-fd2a-7e01fcd62b72"
      },
      "source": [
        "#Attaching embeddings \n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "#print(model.embedding.weight.data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([13821, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7kZrGt55WEq"
      },
      "source": [
        "#@Training the model\n",
        "def train(model, iterator, criterion, optimizer, num_epochs=10):\n",
        "    \n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.review.T).squeeze(1)\n",
        "        loss = criterion(predictions,batch.sentiment.long())\n",
        "\n",
        "        #ORIGINAL (based partly on class code)\n",
        "        #predictions = model(batch.review.T).squeeze(1)\n",
        "        #loss = -torch.index_select(predictions, 1, batch.sentiment.long()).sum()\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        #for name, param in model.named_parameters():\n",
        "        #  print(name, param.grad.abs().sum())\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {epoch_loss:.3f}') \n",
        "\n",
        "  return epoch_loss\n",
        "\n",
        "\n",
        "        #print('Batch size x review (e.g., sent_length)', batch.review.T.shape)\n",
        "        #loss = criterion(predictions, batch.sentiment)\n",
        "        #print('Loss where', loss)\n",
        "        #print('Loss sum', loss)\n",
        "        #print('Loss type', type(loss))\n",
        "        #print('Loss shape', loss.shape)\n",
        "        #print('Predictions shape', predictions.shape)\n",
        "        #print('Predictions type', type(predictions))\n",
        "        #print('sentiment shape', batch.sentiment.shape)\n",
        "        #print('batch.sentiment', batch.sentiment) \n",
        "\n",
        " "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNMPPTBV5vF2",
        "outputId": "eb6a7ed5-43e7-4b5b-a010-9bc58ef7c603"
      },
      "source": [
        "#Printing epochs\n",
        "train_loss = train(model, train_iterator, criterion, optimizer)\n",
        "\n",
        "#Saving final epoch \n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 415.386\n",
            "| Epoch: 02 | Train Loss: 257.977\n",
            "| Epoch: 03 | Train Loss: 171.941\n",
            "| Epoch: 04 | Train Loss: 136.744\n",
            "| Epoch: 05 | Train Loss: 110.160\n",
            "| Epoch: 06 | Train Loss: 108.188\n",
            "| Epoch: 07 | Train Loss: 100.935\n",
            "| Epoch: 08 | Train Loss: 100.114\n",
            "| Epoch: 09 | Train Loss: 106.075\n",
            "| Epoch: 10 | Train Loss: 110.077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFxcVHtTkrJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851d36ec-a057-4d20-d331-3f9aacc46a6f"
      },
      "source": [
        "#@Testing the model\n",
        "\n",
        "#Step 1: Instantiating Model\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTa_cG6XZ2ug"
      },
      "source": [
        "# Last questions:\n",
        "# - Originally, BiLSTM output log_softmax (and manually implementing loss). Our understanding is that log_softmax provides the continuity for backward(). \n",
        "# - However, it must have been incorrectly implemented in terms of getting correct class from probabilities.\n",
        "# - We made it work by taking out log_softmax and using nn.CrossEntropyLoss.\n",
        "\n",
        "# - We decided to use sigmoid for getting prediction probabilities (though theoretically, we should have been able to use softmax), for ease of dealing with types.\n",
        "\n",
        "# - Why is it that, even though we were confident in our class model (in terms of layers), the way we structured our output from our class model caused problems down the pipeline?\n",
        "#   -(a) Why did manual cross entropy loss cause huge, fluctuating training loss magnitudes?\n",
        "#   -(b) Why did this lead to very poor accuracy scores (~5%)? Which didn't change even if we switched labels?\n",
        "\n",
        "# - Please explain labelling to us (particularly, can you confirm if they are switched, and whether the model is learning the switched versions or not)?\n",
        "# - Our accuracy scores as is use the so-called \"switched\" labels, but the acc rate is ~78%, which implies that model is learning the switched version...\n",
        "\n",
        "# - Was the model properly loaded (and is there another way that is preferable)?\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  acc=0\n",
        "  acc_score=0\n",
        "  iterator_len=0\n",
        "  for batch in test_iterator:\n",
        "    pred = model(batch.review.T)\n",
        "    y_hat = torch.sigmoid(pred) #Turn into probabilities\n",
        "    y_hat = torch.argmax(y_hat,dim=1)\n",
        "    #y_hat=torch.where(pred[:,0]>0.5, 1, 0) # Changing labels doesn't change accuracy?\n",
        "    #y_hat=torch.where(pred[:,0]>math.log(0.5), 1, 0)\n",
        "    acc=torch.where(y_hat==batch.sentiment, 1, 0).sum()\n",
        "    acc_score = acc_score + acc\n",
        "\n",
        "acc_score=100*acc_score/(10*len(test_iterator))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI2lak_r5gnf",
        "outputId": "fb3f6890-608b-4f76-c411-fdde41f76c51"
      },
      "source": [
        "print(acc_score)\n",
        "# 78% with reverse labels?"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(73.8798, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}