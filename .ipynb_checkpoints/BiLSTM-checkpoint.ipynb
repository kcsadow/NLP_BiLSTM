{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fromScratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECgnAeptfOJM"
      },
      "source": [
        "# Read Me: \n",
        "The following was a project for CS6741. The goal was to build a pytorch bidirectional LSTM from scratch and achieve state of the art model accuracy for sentiment classification using SST2. This is a work-in-progress, with adjustments needed to achieve higher accuracy.\n",
        "\n",
        "1. Import packages and mount drive\n",
        "2. Bring in GloVe embeddings and seniment data\n",
        "3. Create model\n",
        "4. Set hyperparameters\n",
        "5. Train and test model\n",
        "6. Graph epochs\n",
        "\n",
        "Accuracy is well below the ideal at only 82%. We are considering adjusting the model to add an additional dropout layer, and performing a more rigorous hyperparameter search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzl0dXWqegVo"
      },
      "source": [
        "# 1. Import packages and mount drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKuTb2_4hHX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5145758-d661-4004-c2b1-0a1260bbf582"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "drive_folder = \"gdrive/My Drive/CS6741 Replication Project/\" \n",
        "#drive_folder = \"gdrive/My Drive/CS6741 - Topics in Natural Language Processing and Machine Learning/CS6741 Replication Project/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwjf3IQkhZnQ"
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torchtext import datasets\n",
        "from torchtext.legacy import data\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import altair as alt\n",
        "import spacy \n",
        "import math\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19u_sU9a9BC0",
        "outputId": "7361b296-32a8-46a3-9e70-0117152ceb1e"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        " \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)   \n",
        "\n",
        "device=get_default_device()\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4uRXglqelti"
      },
      "source": [
        "#2. Bring in GloVe embeddings and sentiment data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wth4dccFhb0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3068ad7d-d572-4a7d-a57c-88e04ff1883d"
      },
      "source": [
        "#Glove files\n",
        "glove = torchtext.vocab.GloVe(name='6B',dim=300)\n",
        "print(glove.vectors.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.33MB/s]                           \n",
            "100%|█████████▉| 399169/400000 [00:38<00:00, 10743.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([400000, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4M5mXMNht3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "f37658a2-f3e3-4d5b-b09e-642b4b439aba"
      },
      "source": [
        "#Sentiment files (2 Class from https://github.com/clairett/pytorch-sentiment-classification)\n",
        "colnames=['review', 'sentiment'] \n",
        "train=pd.read_csv(drive_folder+\"data/SST2/train.tsv\", sep = '\\t', names=colnames, header=None)\n",
        "dev=pd.read_csv(drive_folder+\"data/SST2/dev.tsv\", sep = '\\t', names=colnames, header=None)\n",
        "test=pd.read_csv(drive_folder+\"data/SST2/test.tsv\", sep = '\\t', names=colnames, header=None)\n",
        "\n",
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one long string of cliches</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>if you 've ever entertained the notion of doin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>k 19 exploits our substantial collective fear ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it 's played in the most straight faced fashio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>there is a fabric of complex ideas here , and ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0                         one long string of cliches          0\n",
              "1  if you 've ever entertained the notion of doin...          0\n",
              "2  k 19 exploits our substantial collective fear ...          0\n",
              "3  it 's played in the most straight faced fashio...          0\n",
              "4  there is a fabric of complex ideas here , and ...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZVPocnohyC4"
      },
      "source": [
        "#Update preprocessing with our thoughts (pulled from )\n",
        "contraction_dict = {\n",
        "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
        "    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "    \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
        "    \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i will\",  \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "    \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
        "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "def clean_contractions(text, contraction_dict):\n",
        "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "    for s in specials:\n",
        "        text = text.replace(s, \"'\")\n",
        "    text = ' '.join([contraction_dict[t] if t in contraction_dict else t for t in text.split(\" \")])\n",
        "    return text\n",
        "\n",
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "punct_dict = {\n",
        "    \"‘\": \"'\",    \"₹\": \"e\",      \"´\": \"'\", \"°\": \"\",         \"€\": \"e\",\n",
        "    \"™\": \"tm\",   \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\",        \"—\": \"-\",\n",
        "    \"–\": \"-\",    \"’\": \"'\",      \"_\": \"-\", \"`\": \"'\",        '“': '\"',\n",
        "    '”': '\"',    '“': '\"',      \"£\": \"e\", '∞': 'infinity', 'θ': 'theta',\n",
        "    '÷': '/',    'α': 'alpha',  '•': '.', 'à': 'a',        '−': '-',\n",
        "    'β': 'beta', '∅': '',       '³': '3', 'π': 'pi'\n",
        "}\n",
        "def clean_special_chars(text, punct, punct_dict):\n",
        "    for p in punct_dict:\n",
        "        text = text.replace(p, punct_dict[p])\n",
        "    \n",
        "    for p in punct:\n",
        "        text = text.replace(p, f' {p} ')\n",
        "    \n",
        "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
        "    for s in specials:\n",
        "        text = text.replace(s, specials[s])\n",
        "    \n",
        "    return text\n",
        "\n",
        "def preprocess_text(text, contraction_dict, punct, punct_dict):\n",
        "    clean_text=text.lower()\n",
        "    clean_text=clean_contractions(clean_text, contraction_dict)\n",
        "    clean_text=clean_special_chars(clean_text, punct, punct_dict)\n",
        "    clean_text=re.split('\\W+', clean_text)\n",
        "    #clean_text=[token for token in clean_text if token not in stopwords]  \n",
        "    return \" \".join(clean_text)\n",
        "\n",
        "preprocess_text(\"samhdbei. 2345324@@# !~~~ sdne @ dsecwAADEk. SDKM\",contraction_dict, punct, punct_dict)\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser','tagger','ner'])\n",
        "\n",
        "def tokenizer(s):\n",
        "    return [w.text.lower() for w in nlp(preprocess_text(s,contraction_dict, punct, punct_dict))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io0RoRXc7hv4",
        "outputId": "662a1177-3728-4d8a-f3d1-b2cb8f801e19"
      },
      "source": [
        "#Glove + SST --> tensor batch size x review length \n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(tokenize = tokenizer)\n",
        "LABEL = torchtext.legacy.data.LabelField(dtype = torch.float)\n",
        "\n",
        "#reading again using tabular dataset\n",
        "datafields=[('review', TEXT),('sentiment', LABEL)]\n",
        "trn,val,tst=torchtext.legacy.data.TabularDataset.splits(path=drive_folder+\"data/SST2/\", train='train.tsv', validation='dev.tsv', test='test.tsv',format='tsv',skip_header=False, fields=datafields)\n",
        "\n",
        "#including ony top 30000 words from vocab, building vocab for train data \n",
        "# extarcting these words from glove embeddings i.e. unique ids representing words should come from glove\n",
        "# change to higher dimensionaity vector later\n",
        "TEXT.build_vocab(trn,max_size=30000,vectors='glove.6B.300d', unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(trn)\n",
        "\n",
        "#Fix Mappings (they are currently backwards, see next cell)\n",
        "LABEL.vocab.stoi\n",
        "\n",
        "#loop through trn and get a minibatch to work with - creates tensor X \n",
        "train_iterator,test_iterator, val_iterator=torchtext.legacy.data.BucketIterator.splits((trn,tst,val),batch_sizes=(10,10,10),sort_key =lambda x: len(x.review), sort_within_batch=False, device=device)\n",
        "print(len(train_iterator))# train batches\n",
        "print(len(val_iterator))# val batches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "692\n",
            "88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7a2jTcm9Ep4",
        "outputId": "d19e854a-0e3e-4b8b-b0ca-04bdc2fb8236"
      },
      "source": [
        "#Checking batch size\n",
        "def show_batch(dl):\n",
        "    for reviews, sentiments in dl:\n",
        "        print(reviews.T.shape)\n",
        "        print(sentiments.shape)\n",
        "        print(sentiments)\n",
        "        break\n",
        "        \n",
        "show_batch(train_iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20])\n",
            "torch.Size([10])\n",
            "tensor([0., 1., 1., 0., 1., 1., 0., 0., 0., 0.], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VFuF9kTe9N5"
      },
      "source": [
        "# 3. Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AhXBclth5pe"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
        "                 output_dim, n_layers, bidirectional, batch_first, dropout):\n",
        "        \n",
        "        super(BiLSTM, self).__init__()\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # convert sparse 1 hot encoded vectors to embeddings (glove embedding will be used here)\n",
        "        \n",
        "        #hidden_dim -> takes in 300 embedding gives 20 features (if we set hyperparamter hidden_dim = 20)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim,\n",
        "                            num_layers=n_layers, batch_first=batch_first,\n",
        "                            dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim) # this is the W \n",
        "\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "      embedded = self.embedding(text)\n",
        "\n",
        "      embedded = self.dropout(embedded)\n",
        "\n",
        "      _, (hidden, _) = self.lstm(embedded)\n",
        "\n",
        "      #print('LSTM hidden shape', hidden.shape)\n",
        "\n",
        "      #final hidden from left/right and concatenating\n",
        "      hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)  #hidden shape before concatenation: 4x10x100\n",
        "\n",
        "      output = self.fc(hidden) \n",
        "      #print('Linear output shape', output.shape)\n",
        "\n",
        "      #return torch.log_softmax(output, dim=-1) #this is the final phi from class - HAS to be log_softmax\n",
        "      return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rll_2Zo7frPV"
      },
      "source": [
        "#4. Set hyperparameters & attach encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi3zrOZlvbJq"
      },
      "source": [
        "#Setting configurations and instantiating model\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 100 # Ask Sasha\n",
        "output_dim =  2\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "batch_first=True\n",
        "\n",
        "model = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, batch_first, dropout)\n",
        "model.cuda()\n",
        "\n",
        "#Optimizer \n",
        "\n",
        "#Option 1: SGD\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0002) \n",
        "\n",
        "#Option 2: Adam \n",
        "lr=np.linspace(0.0001, 0.01, 10) #grid search (not instantiated yet) | bastings used 0.0002\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=10e-6) # Our choice for best loss\n",
        "\n",
        "#Criterion\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bvPYmAzCXBq",
        "outputId": "92636d7f-970a-433a-f8ab-b591dc0fb59f"
      },
      "source": [
        "#Attaching embeddings \n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "#print(model.embedding.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([13821, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6fYYAASfwjZ"
      },
      "source": [
        "# 5. Train and Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7kZrGt55WEq"
      },
      "source": [
        "#Train\n",
        "def train(model, iterator, criterion, optimizer):\n",
        "    \n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch in iterator:\n",
        "        \n",
        "      optimizer.zero_grad()\n",
        "        \n",
        "      predictions = model(batch.review.T).squeeze(1)\n",
        "      loss = criterion(predictions,batch.sentiment.long())\n",
        "\n",
        "      #ORIGINAL\n",
        "      #predictions = model(batch.review.T).squeeze(1)\n",
        "      #loss = -torch.index_select(predictions, 1, batch.sentiment.long()).mean() #check this out with mean (see cross entropy pytorch - mean by default)\n",
        "        \n",
        "      loss.backward()\n",
        " \n",
        "      optimizer.step()\n",
        "        \n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSmQRHFb8TJV"
      },
      "source": [
        "#Validation\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    #initialize every epoch\n",
        "    epoch_loss = 0\n",
        "\n",
        "    #deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    #deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            #Doing the predictions  \n",
        "            predictions = model(batch.review.T).squeeze(1)\n",
        "            \n",
        "            #compute loss and accuracy\n",
        "            loss = criterion(predictions,batch.sentiment.long())\n",
        "            \n",
        "            #keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDeKmaac_3Sh",
        "outputId": "b3b67867-c99d-4577-d86a-70b9b6958d53"
      },
      "source": [
        "#Comparing train/val in 10 epochs\n",
        "\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    #train the model\n",
        "    train_loss = train(model, train_iterator, criterion, optimizer)\n",
        "    \n",
        "    #evaluate the model\n",
        "    valid_loss = evaluate(model, val_iterator, criterion)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pth')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 374.843%\n",
            "\t Val. Loss: 0.416%\n",
            "\tTrain Loss: 229.723%\n",
            "\t Val. Loss: 0.418%\n",
            "\tTrain Loss: 121.617%\n",
            "\t Val. Loss: 0.497%\n",
            "\tTrain Loss: 63.668%\n",
            "\t Val. Loss: 0.642%\n",
            "\tTrain Loss: 40.980%\n",
            "\t Val. Loss: 0.716%\n",
            "\tTrain Loss: 31.661%\n",
            "\t Val. Loss: 0.879%\n",
            "\tTrain Loss: 29.106%\n",
            "\t Val. Loss: 1.018%\n",
            "\tTrain Loss: 25.492%\n",
            "\t Val. Loss: 0.758%\n",
            "\tTrain Loss: 16.742%\n",
            "\t Val. Loss: 1.035%\n",
            "\tTrain Loss: 18.389%\n",
            "\t Val. Loss: 1.208%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTa_cG6XZ2ug"
      },
      "source": [
        "#Test\n",
        "with torch.no_grad():\n",
        "  acc=0\n",
        "  acc_score=0\n",
        "  iterator_len=0\n",
        "  for batch in test_iterator:\n",
        "    pred = model(batch.review.T)\n",
        "    y_hat = torch.sigmoid(pred) #Turn into probabilities\n",
        "    #print('original',y_hat)\n",
        "    y_hat = torch.argmax(y_hat,dim=1)\n",
        "    #print('argmaxed',y_hat)\n",
        "    #print(batch.sentiment)\n",
        "    #y_hat=torch.where(pred[:,0]>0.5, 1, 0) # Changing labels doesn't change accuracy?\n",
        "    #y_hat=torch.where(pred[:,0]>math.log(0.5), 1, 0)\n",
        "    acc=torch.where(y_hat==batch.sentiment, 1, 0).sum()\n",
        "    acc_score = acc_score + acc\n",
        "\n",
        "acc_score=100*acc_score/(10*len(test_iterator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI2lak_r5gnf",
        "outputId": "f25b1306-6f9e-4de0-987d-f4edd9e6dc22"
      },
      "source": [
        "print(acc_score)\n",
        "# 78% with reverse labels?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(82.0765, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvY09-RAf2vO"
      },
      "source": [
        "# 6. Graph epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUeTH9DSeHQ0"
      },
      "source": [
        "#Graphing epochs\n",
        "data=[]\n",
        "epoch=[]\n",
        "loss=[]\n",
        "epoch=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "loss=[385, 233, 125, 63, 38, 28, 29, 24, 21, 17]\n",
        "data=zip(epoch, loss)\n",
        "df=pd.DataFrame(data, columns=(\"epoch\", \"loss\"))\n",
        "df.to_latex()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdJUf4yPeSVU"
      },
      "source": [
        "chart = (alt.Chart(df)\n",
        "  .mark_line()\n",
        "  .properties(title=\"Loss Across Epochs\")\n",
        "  .encode(x=\"epoch\", y=\"loss\"))\n",
        "chart"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}